{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING QA BOT WITH PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading training data\n",
    "with open('train_qa.txt','rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test data\n",
    "with open('test_qa.txt','rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data contains 10, 000 tuples. Each tuple holds a list comprehension of the story, question, and  a \"yes\"/\"no\" answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Story\n",
    "\" \".join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Sandra in the hallway ?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question\n",
    "\" \".join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_creator(data):\n",
    "    '''\n",
    "    Creating vocabulary of words present in our data\n",
    "    '''\n",
    "    vocab = set()\n",
    "    for story, question, answer in data:\n",
    "        vocab = vocab.union(set(story))\n",
    "        vocab = vocab.union(set(question))\n",
    "    vocab.add('yes')\n",
    "    vocab.add('no')\n",
    "    \n",
    "    tokenizer = Tokenizer(filters = [])\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    \n",
    "    max_story_len = max([len(datum[0]) for datum in data]) #maximum story length\n",
    "    max_question_len = max([len(datum[1]) for datum in data]) #maximum question length\n",
    "    \n",
    "    return tokenizer, max_story_len, max_question_len, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8db5f99d9f44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m def vectorize_stories(data, word_index = tokenizer.word_index, \n\u001b[0m\u001b[0;32m      2\u001b[0m                       \u001b[0mmax_story_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_story_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                       max_question_len = max_question_len):\n\u001b[0;32m      4\u001b[0m     '''\n\u001b[0;32m      5\u001b[0m     \u001b[0mVectorizes\u001b[0m \u001b[0mstory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mpadded\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def vectorize_stories(data, word_index = tokenizer.word_index, \n",
    "                      max_story_len = max_story_len, \n",
    "                      max_question_len = max_question_len):\n",
    "    '''\n",
    "    Vectorizes story, question, and answer with padded sequences\n",
    "    X = Stories\n",
    "    X_Q = Questions\n",
    "    Y  = Answers\n",
    "    '''\n",
    "    X  = []\n",
    "    X_Q = []\n",
    "    Y = []\n",
    "    \n",
    "    for stories ,questions, answers in data:\n",
    "        '''\n",
    "        For each story\n",
    "        [23,14,43,...]\n",
    "        '''\n",
    "        x = [word_index[word.lower()] for word in stories]\n",
    "        x_q = [word_index[word.lower()] for word in questions]\n",
    "        \n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        y[word_index[answers]] = 1\n",
    "        \n",
    "        X.append(x)\n",
    "        X_Q.append(x_q)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen = max_story_len), pad_sequences(X_Q, maxlen = max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(train_data, test_data):\n",
    "    all_data = train_data + test_data # Combine train data and test data\n",
    "    \n",
    "    tokenizer, max_story_len, max_question_len, vocab = vocab_creator(all_data)    \n",
    "    vocab_size = len(vocab) + 1 #size of our vocabulary\n",
    "    \n",
    "    input_sequence = Input((max_story_len,)) #placeholder for shape = (max_story_len, batch_size)\n",
    "    question = Input((max_question_len,))\n",
    "    \n",
    "    return vocab_size, input_sequence, question, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, input_sequence, question, tokenizer = data_preprocessing(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(vocab_size = vocab_size, max_question_len = max_question_len, input_sequence = input_sequence, question = question):\n",
    "    '''\n",
    "    Input Encoder M\n",
    "    Gets embedded to a seq of vecotrs\n",
    "    '''\n",
    "    input_encoder_m = Sequential()\n",
    "    input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim = 64))\n",
    "    input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "    '''\n",
    "    Input Encoder C\n",
    "    Gets embedded to a seq of vecotrs\n",
    "    '''\n",
    "    input_encoder_c = Sequential()\n",
    "    input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim = max_question_len))\n",
    "    input_encoder_c.add(Dropout(.3))\n",
    "    \n",
    "    '''\n",
    "    Question Encoder\n",
    "    '''\n",
    "    question_encoder = Sequential()\n",
    "    question_encoder.add(Embedding(input_dim=vocab_size, output_dim = 64, input_length = max_question_len))\n",
    "    question_encoder.add(Dropout(.3))\n",
    "    \n",
    "    '''\n",
    "    ENCODED RESULT <-- ENCODER(INPUT)\n",
    "    '''\n",
    "    input_encoded_m = input_encoder_m(input_sequence)\n",
    "    input_encoded_c = input_encoder_c(input_sequence)\n",
    "    question_encoded =  question_encoder(question)\n",
    "\n",
    "    '''\n",
    "    Use dot product to compute the match between the first input vector\n",
    "    seq and the query\n",
    "    '''\n",
    "    match = dot([input_encoded_m, question_encoded], axes =(2,2))\n",
    "    match = Activation('softmax')(match)\n",
    "    \n",
    "    '''\n",
    "    Add above match matrix to second input vector seq\n",
    "    '''\n",
    "    response = add([match, input_encoded_c])\n",
    "    response = Permute((2,1))(response) # convert it to have a output of samples dim by query_max_len by story_max_len\n",
    "    \n",
    "    '''\n",
    "    Now we can concat the match matrix with the question vector seq\n",
    "    '''\n",
    "    answer = concatenate([response, question_encoded])\n",
    "    answer = LSTM(32)(answer)\n",
    "    answer = Dropout(0.5)(answer)\n",
    "    answer = Dense(vocab_size)(answer) # (sample, vocab_size)\n",
    "    answer = Activation('softmax')(answer)\n",
    "    model = Model([input_sequence, question], answer)\n",
    "    model.compile(optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([inputs_train, queries_train], answers_train, batch_size = 32, epochs = 40, validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model\n",
    "#### Plotting Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "#summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('qachatbot.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create my own story and question to test the model on. Keep in mind that our model was trained on the words in the \"vocab\" *only*, so in creating our own story, we will have to stick to words in the vocabulary.\n",
    "\n",
    "Also, keep a space between punctuations so as to recreate the input we trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ , _, vocab = vocab_creator(train_data + test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story = \"Mary dropped the milk in the kitchen . Mary went to the office . \"\n",
    "my_question = \"Is Mary in the office ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [(my_story.split(), my_question.split(), 'no')]\n",
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story, my_question, my_answer = vectorize_stories(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = model.predict(([my_story, my_question]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_max = np.argmax(predict_result[0])\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result[0][val_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
