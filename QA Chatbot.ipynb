{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING QA BOT WITH PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading training data\n",
    "with open('train_qa.txt','rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test data\n",
    "with open('test_qa.txt','rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.'],\n",
       " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
       " 'no')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data contains 10, 000 tuples. Each tuple holds a list comprehension of the story, question, and  a \"yes\"/\"no\" answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Story\n",
    "\" \".join(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is Sandra in the hallway ?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question\n",
    "\" \".join(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "train_data[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Combine train and test data \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_creator(data):\n",
    "    '''\n",
    "    Creating vocabulary of words present in our data\n",
    "    '''\n",
    "    vocab = set()\n",
    "    for story, question, answer in data:\n",
    "        vocab = vocab.union(set(story))\n",
    "        vocab = vocab.union(set(question))\n",
    "    vocab.add('yes')\n",
    "    vocab.add('no')\n",
    "    \n",
    "    tokenizer = Tokenizer(filters = [])\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    \n",
    "    max_story_len = max([len(datum[0]) for datum in data]) #maximum story length\n",
    "    max_question_len = max([len(datum[1]) for datum in data]) #maximum question length\n",
    "    \n",
    "    return tokenizer, max_story_len, max_question_len, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_index = tokenizer.word_index, \n",
    "                      max_story_len = max_story_len, \n",
    "                      max_question_len = max_question_len):\n",
    "    '''\n",
    "    Vectorizes story, question, and answer with padded sequences\n",
    "    X = Stories\n",
    "    X_Q = Questions\n",
    "    Y  = Answers\n",
    "    '''\n",
    "    X  = []\n",
    "    X_Q = []\n",
    "    Y = []\n",
    "    \n",
    "    for stories ,questions, answers in data:\n",
    "        '''\n",
    "        For each story\n",
    "        [23,14,43,...]\n",
    "        '''\n",
    "        x = [word_index[word.lower()] for word in stories]\n",
    "        x_q = [word_index[word.lower()] for word in questions]\n",
    "        \n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        y[word_index[answers]] = 1\n",
    "        \n",
    "        X.append(x)\n",
    "        X_Q.append(x_q)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen = max_story_len), pad_sequences(X_Q, maxlen = max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(train_data, test_data):\n",
    "    all_data = train_data + test_data # Combine train data and test data\n",
    "    \n",
    "    tokenizer, max_story_len, max_question_len, vocab = vocab_creator(all_data)    \n",
    "    vocab_size = len(vocab) + 1 #size of our vocabulary\n",
    "    \n",
    "    \n",
    "    inputs_train, queries_train, answers_train = vectorize_stories(train_data)\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(test_data)\n",
    "    \n",
    "    input_sequence = Input((max_story_len,)) #placeholder for shape = (max_story_len, batch_size)\n",
    "    question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    '''\n",
    "    Input Encoder M\n",
    "    Gets embedded to a seq of vecotrs\n",
    "    '''\n",
    "    input_encoder_m = Sequential()\n",
    "    input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim = 64))\n",
    "    input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "    '''\n",
    "    Input Encoder C\n",
    "    Gets embedded to a seq of vecotrs\n",
    "    '''\n",
    "    input_encoder_c = Sequential()\n",
    "    input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim = max_question_len))\n",
    "    input_encoder_c.add(Dropout(.3))\n",
    "    \n",
    "    '''\n",
    "    Question Encoder\n",
    "    '''\n",
    "    question_encoder = Sequential()\n",
    "    question_encoder.add(Embedding(input_dim=vocab_size, output_dim = 64, input_length = max_question_len))\n",
    "    question_encoder.add(Dropout(.3))\n",
    "    \n",
    "    '''\n",
    "    ENCODED RESULT <-- ENCODER(INPUT)\n",
    "    '''\n",
    "    input_encoded_m = input_encoder_m(input_sequence)\n",
    "    input_encoded_c = input_encoder_c(input_sequence)\n",
    "    question_encoded =  question_encoder(question)\n",
    "\n",
    "    '''\n",
    "    Use dot product to compute the match between the first input vector\n",
    "    seq and the query\n",
    "    '''\n",
    "    match = dot([input_encoded_m, question_encoded], axes =(2,2))\n",
    "    match = Activation('softmax')(match)\n",
    "    \n",
    "    '''\n",
    "    Add above match matrix to second input vector seq\n",
    "    '''\n",
    "    response = add([match, input_encoded_c])\n",
    "    response = Permute((2,1))(response) # convert it to have a output of samples dim by query_max_len by story_max_len\n",
    "    \n",
    "    '''\n",
    "    Now we can concat the match matrix with the question vector seq\n",
    "    '''\n",
    "    answer = concatenate([response, question_encoded])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
